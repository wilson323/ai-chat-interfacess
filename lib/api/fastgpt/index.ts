/**
 * FastGPT API ç»Ÿä¸€å¤„ç†æ¨¡å—
 * æ•´åˆäº†ä¹‹å‰åˆ†æ•£åœ¨fastgpt.tså’Œfastgpt-client.tsçš„åŠŸèƒ½
 */

import type { Agent } from "../../../types/agent"
import type { Message } from "../../../types/message"
import { retry } from "../../utils/index"
import { API_CONSTANTS, ERROR_MESSAGES } from "../../storage/shared/constants"
import { DEFAULT_AGENT_SETTINGS } from "@/lib/storage/shared/constants"
import { logFastGPTEvent, logProcessingStep, logApiRequest, logApiResponse, logError } from "../../debug-utils"
import {
  createCrossPlatformTextDecoder,
  createCrossPlatformTextEncoder,
  isStreamingContentType,
  processStreamLines,
  categorizeStreamError,
  safeCrossPlatformLog
} from "@/lib/cross-platform-utils"
import type {
  ChatCompletionRequest,
  ChatCompletionResponse,
  ChatCompletionChunk,
  InitChatRequest,
  InitChatResponse,
  ChatSession,
  ChatMessage,
  MessageFeedbackRequest,
  MessageFeedbackResponse
} from '../../../types/api/fastgpt';

// ================ ç±»å‹å®šä¹‰ ================

export interface StreamOptions {
  temperature?: number
  maxTokens?: number
  detail?: boolean
  onStart?: () => void
  onChunk?: (chunk: string) => void
  onIntermediateValue?: (value: any, eventType: string) => void
  onProcessingStep?: (step: any) => void
  onError?: (error: Error) => void
  onFinish?: () => void
  signal?: AbortSignal
  variables?: Record<string, any> // æ–°å¢å…¨å±€å˜é‡æ”¯æŒ
}

export interface FastGPTChatResponse {
  id: string
  object: string
  created: number
  model: string
  choices: {
    index: number
    message: {
      role: string
      content: string
      tool_calls?: any[]
    }
    finish_reason: string
  }[]
  usage?: {
    prompt_tokens: number
    completion_tokens: number
    total_tokens: number
  }
  detail?: any
}

export interface FastGPTStreamChunk {
  id: string
  object: string
  created: number
  model: string
  choices: {
    index: number
    delta: {
      content?: string
      role?: string
      tool_calls?: any[]
    }
    finish_reason: string | null
  }[]
}

export interface ChatHistoryRecord {
  _id: string
  dataId: string
  obj: "Human" | "AI"
  value: {
    type: string
    text?: {
      content: string
    }
    image?: {
      url: string
    }
  }[]
  customFeedbacks: any[]
  llmModuleAccount?: number
  totalQuoteList?: any[]
  totalRunningTime?: number
  historyPreviewLength?: number
}

export interface ChatHistoryResponse {
  code: number
  data: {
    list: ChatHistoryRecord[]
    total: number
  }
}

export interface ChatInitResponse {
  code: number
  data: {
    chatId: string
    appId: string
    variables: Record<string, any>
    app: {
      chatConfig: {
        questionGuide: boolean
        ttsConfig: {
          type: string
        }
        whisperConfig: {
          open: boolean
          autoSend: boolean
          autoTTSResponse: boolean
        }
        chatInputGuide: {
          open: boolean
          textList: string[]
          customUrl: string
        }
        instruction: string
        variables: any[]
        fileSelectConfig: {
          canSelectFile: boolean
          canSelectImg: boolean
          maxFiles: number
        }
        _id: string
        welcomeText: string
      }
      chatModels: string[]
      name: string
      avatar: string
      intro: string
      type: string
      pluginInputs: any[]
    }
    interacts?: any[] // æ·»åŠ å¯é€‰çš„interactså­—æ®µ
  }
}

export interface QuestionGuideResponse {
  code: number
  data: string[]
}

// ================ å·¥å…·å‡½æ•° ================

/**
 * ç¡®å®šæ˜¯å¦åº”è¯¥ä½¿ç”¨ä»£ç†
 */
function shouldUseProxy(): boolean {
  // åœ¨æµè§ˆå™¨ç¯å¢ƒä¸­å§‹ç»ˆä½¿ç”¨ä»£ç†
  if (typeof window !== "undefined") {
    return true
  }
  return false
}

/**
 * ç”Ÿæˆæœ¬åœ°å›é€€èŠå¤©ID
 */
export function generateFallbackChatId(): string {
  return `local_${Date.now()}_${Math.random().toString(36).substring(2, 9)}`
}

/**
 * ç”Ÿæˆå›é€€èŠå¤©å“åº”
 */
function generateFallbackChatResponse(message: string, model: string): FastGPTChatResponse {
  return {
    id: `local-${Date.now()}`,
    object: "chat.completion",
    created: Math.floor(Date.now() / 1000),
    model: model,
    choices: [
      {
        index: 0,
        message: {
          role: "assistant",
          content: message,
        },
        finish_reason: "error",
      },
    ],
    usage: {
      prompt_tokens: 0,
      completion_tokens: 0,
      total_tokens: 0,
    },
  }
}

/**
 * ç”Ÿæˆå›é€€åˆå§‹åŒ–å“åº”
 */
export function generateFallbackResponse(agent: Agent, chatId: string): ChatInitResponse {
  // ä¸ºä¸åŒç±»å‹çš„æ™ºèƒ½ä½“ç”Ÿæˆä¸åŒçš„æ¬¢è¿æ¶ˆæ¯
  let welcomeMessage = "æ‚¨å¥½ï¼æˆ‘æ˜¯æ™ºèƒ½åŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚è¯·é—®æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„ï¼Ÿ"
  let interacts: string[] = []

  if (agent.type === "image-editor") {
    welcomeMessage = "æ¬¢è¿ä½¿ç”¨å›¾åƒç¼–è¾‘åŠ©æ‰‹ï¼æ‚¨å¯ä»¥ä¸Šä¼ å›¾ç‰‡ï¼Œæˆ‘å°†å¸®åŠ©æ‚¨è¿›è¡Œç¼–è¾‘å’Œå¤„ç†ã€‚"
    interacts = [
      "å¦‚ä½•è£å‰ªå›¾ç‰‡ï¼Ÿ",
      "èƒ½å¸®æˆ‘è°ƒæ•´å›¾ç‰‡äº®åº¦å—ï¼Ÿ",
      "å¦‚ä½•æ·»åŠ æ»¤é•œæ•ˆæœï¼Ÿ",
      "èƒ½å¸®æˆ‘å»é™¤å›¾ç‰‡èƒŒæ™¯å—ï¼Ÿ",
      "å¦‚ä½•è°ƒæ•´å›¾ç‰‡å¤§å°ï¼Ÿ",
    ]
  } else if (agent.type === "cad-analyzer") {
    welcomeMessage = "æ¬¢è¿ä½¿ç”¨CADåˆ†æåŠ©æ‰‹ï¼æ‚¨å¯ä»¥ä¸Šä¼ CADå›¾çº¸ï¼Œæˆ‘å°†å¸®åŠ©æ‚¨åˆ†æå…¶ä¸­çš„å®‰é˜²è®¾å¤‡å¸ƒå±€ã€‚"
    interacts = [
      "å¦‚ä½•åˆ†æCADå›¾çº¸ä¸­çš„å®‰é˜²è®¾å¤‡ï¼Ÿ",
      "èƒ½è¯†åˆ«å›¾çº¸ä¸­çš„æ‘„åƒå¤´ä½ç½®å—ï¼Ÿ",
      "å¦‚ä½•è®¡ç®—å¸ƒçº¿é•¿åº¦ï¼Ÿ",
      "èƒ½å¸®æˆ‘ä¼˜åŒ–è®¾å¤‡å¸ƒå±€å—ï¼Ÿ",
      "å¦‚ä½•å¯¼å‡ºåˆ†ææŠ¥å‘Šï¼Ÿ",
    ]
  } else {
    // é»˜è®¤èŠå¤©æ™ºèƒ½ä½“
    interacts = ["ä½ èƒ½åšä»€ä¹ˆï¼Ÿ", "ä»‹ç»ä¸€ä¸‹ä½ çš„åŠŸèƒ½", "å¦‚ä½•ä½¿ç”¨ä½ çš„æœåŠ¡ï¼Ÿ", "ä½ æœ‰å“ªäº›é™åˆ¶ï¼Ÿ", "èƒ½ç»™æˆ‘ä¸€äº›ä½¿ç”¨ç¤ºä¾‹å—ï¼Ÿ"]
  }

  // ä¼˜å…ˆä½¿ç”¨agentä¸­çš„welcomeTextï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤æ¬¢è¿æ¶ˆæ¯
  const finalWelcomeMessage = agent.welcomeText || welcomeMessage
  // ä¼˜å…ˆä½¿ç”¨agentä¸­çš„systemPromptï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤ç³»ç»Ÿæç¤ºè¯
  const finalSystemPrompt = agent.systemPrompt || "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·è€å¿ƒè§£ç­”ç”¨æˆ·é—®é¢˜ã€‚"

  return {
    code: 200,
    data: {
      chatId: chatId,
      appId: agent.appId || "fallback-app-id",
      variables: {},
      app: {
        chatConfig: {
          questionGuide: true,
          ttsConfig: { type: "normal" },
          whisperConfig: { open: false, autoSend: false, autoTTSResponse: false },
          chatInputGuide: { open: false, textList: [], customUrl: "" },
          instruction: "",
          variables: [],
          fileSelectConfig: { canSelectFile: false, canSelectImg: false, maxFiles: 5 },
          _id: "",
          welcomeText: finalWelcomeMessage,
        },
        chatModels: [agent.multimodalModel || "gpt-3.5-turbo"],
        name: agent.name || "AI Assistant",
        avatar: "",
        intro: agent.description || "",
        type: "chat",
        pluginInputs: [],
      },
      interacts: interacts,
    },
  }
}

/**
 * è·å–é»˜è®¤é—®é¢˜å»ºè®®
 */
function getDefaultSuggestions(): string[] {
  return ["è¿™ä¸ªäº§å“æœ‰å“ªäº›åŠŸèƒ½ï¼Ÿ", "å¦‚ä½•ä½¿ç”¨è¿™ä¸ªç³»ç»Ÿï¼Ÿ", "æœ‰æ²¡æœ‰ç›¸å…³çš„ä½¿ç”¨æ¡ˆä¾‹ï¼Ÿ", "èƒ½æä¾›ä¸€äº›ç¤ºä¾‹å—ï¼Ÿ", "æœ‰å“ªäº›é™åˆ¶ï¼Ÿ"]
}

// ================ FastGPTå®¢æˆ·ç«¯ç±» ================

/**
 * FastGPTå®¢æˆ·ç«¯ç±»
 * æä¾›é¢å‘å¯¹è±¡çš„APIè®¿é—®æ–¹å¼
 */
export class FastGPTClient {
  private agent: Agent
  private retryOptions: {
    maxRetries: number
    onRetry?: (attempt: number, error: Error) => void
  }

  constructor(
    agent: Agent,
    retryOptions?: {
      maxRetries?: number
      onRetry?: (attempt: number, error: Error) => void
    },
  ) {
    // ç¡®ä¿agentä½¿ç”¨æ­£ç¡®çš„APIç«¯ç‚¹
    this.agent = {
      ...agent,
      apiEndpoint: API_CONSTANTS.FASTGPT_API_ENDPOINT,
    }
    this.retryOptions = {
      maxRetries: retryOptions?.maxRetries || API_CONSTANTS.DEFAULT_MAX_RETRIES,
      onRetry: retryOptions?.onRetry,
    }
  }

  /**
   * åˆå§‹åŒ–èŠå¤©ä¼šè¯
   */
  async initializeChat(chatId?: string): Promise<ChatInitResponse> {
    return initializeChat(this.agent, chatId)
  }

  /**
   * å‘é€èŠå¤©è¯·æ±‚åˆ° FastGPT API
   * æ”¯æŒæµå¼å“åº”å’Œè‡ªåŠ¨é‡è¯•
   */
  async streamChat(messages: any[], options: StreamOptions): Promise<void> {
    let attempt = 0
    const maxRetries = 2
    let lastError: any = null
    while (attempt <= maxRetries) {
      try {
        await this._streamChatInternal(messages, options)
        return
      } catch (err) {
        lastError = err
        attempt++
        if (attempt > maxRetries) throw lastError
        // æ–­çº¿é‡è¿æç¤ºï¼Œå¯åŠ æ—¥å¿—
      }
    }
  }

  /**
   * å†…éƒ¨å®ç°çš„æµå¼èŠå¤©è¯·æ±‚
   */
  private async _streamChatInternal(
    messages: Array<{ role: string; content: string }>,
    options: StreamOptions = {},
  ): Promise<void> {
    // ä½¿ç”¨å¸¸é‡APIç«¯ç‚¹
    const apiEndpoint = API_CONSTANTS.FASTGPT_API_ENDPOINT

    // æ£€æŸ¥APIç«¯ç‚¹æ˜¯å¦é…ç½®
    if (!apiEndpoint) {
      const error = new Error(ERROR_MESSAGES.INCOMPLETE_CONFIG)
      if (options.onError) {
        options.onError(error instanceof Error ? error : new Error(String(error)))
      }
      throw error
    }

    // å¦‚æœAPIå¯†é’¥æˆ–AppIDæœªé…ç½®ï¼Œä½¿ç”¨ç¦»çº¿æ¨¡å¼
    if (!this.agent.apiKey || !this.agent.appId) {
      console.warn("API key or AppID is not configured, using offline mode")

      // é€šçŸ¥å¼€å§‹æµå¼ä¼ è¾“
      if (options.onStart) {
        options.onStart()
      }

      // ç”Ÿæˆç¦»çº¿å“åº”
      const offlineResponse = "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ã€‚è¯·ç¡®ä¿æ‚¨å·²é…ç½®APIå¯†é’¥å’ŒAppIDï¼Œæˆ–è”ç³»ç®¡ç†å‘˜è·å–å¸®åŠ©ã€‚"

      // å‘é€ç¦»çº¿å“åº”
      if (options.onChunk) {
        options.onChunk(offlineResponse)
      }

      // å®Œæˆæµå¼ä¼ è¾“
      if (options.onFinish) {
        options.onFinish()
      }

      return
    }

    // ç¡®ä¿æœ‰æœ‰æ•ˆçš„ chatId
    if (!this.agent.chatId) {
      this.agent.chatId = generateFallbackChatId()
    }

    // å‡†å¤‡è¯·æ±‚ä½“ï¼Œç¬¦åˆ FastGPT API æ ¼å¼
    const requestBody = {
      model: this.agent.appId, // ä½¿ç”¨ appId ä½œä¸ºæ¨¡å‹å‚æ•°
      chatId: this.agent.chatId,
      stream: true, // å¼ºåˆ¶æµå¼
      detail: true, // å¼ºåˆ¶ detail
      messages: messages,
      temperature: options.temperature || this.agent.temperature || DEFAULT_AGENT_SETTINGS.temperature,
      max_tokens: options.maxTokens || this.agent.maxTokens || DEFAULT_AGENT_SETTINGS.maxTokens,
      variables: options.variables || {}, // æ·»åŠ å…¨å±€å˜é‡æ”¯æŒ
    }

    // é€šçŸ¥å¼€å§‹æµå¼ä¼ è¾“
    if (options.onStart) {
      options.onStart()
    }

    // ä½¿ç”¨ä»£ç† API è¿›è¡Œæµå¼ä¼ è¾“
    const proxyData = {
      targetUrl: apiEndpoint,
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${this.agent.apiKey}`,
        Accept: "text/event-stream",
      },
      body: requestBody,
    }

    // æ·»åŠ è¶…æ—¶å¤„ç†å’Œå¤–éƒ¨signalæ”¯æŒ
    const controller = new AbortController()
    const timeoutId = setTimeout(() => controller.abort(), API_CONSTANTS.STREAM_TIMEOUT)
    const signal = options.signal || controller.signal

    try {
      console.log("é€šè¿‡ä»£ç†å‘é€æµå¼è¯·æ±‚åˆ°:", apiEndpoint)
      console.log("è¯·æ±‚ä½“(éƒ¨åˆ†):", JSON.stringify(requestBody).substring(0, 200) + "...")

      // è®°å½•APIè¯·æ±‚
      logApiRequest("POST", apiEndpoint, requestBody)

      const response = await fetch("/api/chat-proxy", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify(proxyData),
        signal,
      })
      console.log('response-----------=',response)

      clearTimeout(timeoutId)

      if (!response.ok) {
        const errorText = await response.text().catch(() => "æ— æ³•è¯»å–é”™è¯¯å“åº”")
        console.error(`API è¯·æ±‚å¤±è´¥: ${response.status} ${response.statusText} - ${errorText}`)

        const error = new Error(`API è¯·æ±‚å¤±è´¥: ${response.status} ${response.statusText}`)
        if (options.onError) {
          options.onError(error instanceof Error ? error : new Error(String(error)))
        }
        throw error
      }

      // è®°å½•APIå“åº”
      logApiResponse(apiEndpoint, response.status, "Stream response")

      // ğŸ”¥ ä½¿ç”¨è·¨å¹³å°å…¼å®¹çš„å†…å®¹ç±»å‹æ£€æŸ¥
      const contentType = response.headers.get("content-type") || ""
      safeCrossPlatformLog('log', `å“åº”å†…å®¹ç±»å‹æ£€æŸ¥`, { contentType })

      if (!isStreamingContentType(contentType)) {
        safeCrossPlatformLog('warn', `é¢„æœŸæµå¼å†…å®¹ä½†æ”¶åˆ°éæ ‡å‡†ç±»å‹`, { contentType })

        // ğŸ”¥ ä¸ç«‹å³æŠ›å‡ºé”™è¯¯ï¼Œå°è¯•è¯»å–å“åº”ä½“åˆ¤æ–­æ˜¯å¦å¯ä»¥å¤„ç†
        try {
          const text = await response.text()
          safeCrossPlatformLog('log', `å°è¯•è§£æéæµå¼å“åº”`, {
            textLength: text.length,
            preview: text.substring(0, 200)
          })

          // å¦‚æœå“åº”ä½“çœ‹èµ·æ¥åƒJSONï¼Œå°è¯•è§£æ
          if (text.trim().startsWith('{') || text.trim().startsWith('[')) {
            const jsonData = JSON.parse(text)
            if (jsonData.choices && jsonData.choices[0] && jsonData.choices[0].message) {
              // è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„éæµå¼å“åº”ï¼Œæ¨¡æ‹Ÿæµå¼è¾“å‡º
              const content = jsonData.choices[0].message.content || ""
              safeCrossPlatformLog('log', `æ¨¡æ‹Ÿæµå¼è¾“å‡ºéæµå¼å“åº”`, { contentLength: content.length })

              if (options.onChunk) {
                // åˆ†å—å‘é€å†…å®¹ä»¥æ¨¡æ‹Ÿæµå¼æ•ˆæœ
                for (let i = 0; i < content.length; i += 10) {
                  const chunk = content.slice(i, i + 10)
                  options.onChunk(chunk)
                  await new Promise(resolve => setTimeout(resolve, 50)) // æ¨¡æ‹Ÿå»¶è¿Ÿ
                }
              }
              if (options.onFinish) {
                options.onFinish()
              }
              return
            }
          }
        } catch (parseError) {
          safeCrossPlatformLog('error', `è§£æéæµå¼å“åº”å¤±è´¥`, parseError)
        }

        // å¦‚æœæ— æ³•å¤„ç†ï¼ŒæŠ›å‡ºé”™è¯¯
        const error = new Error(`ä¸æ”¯æŒçš„å†…å®¹ç±»å‹: ${contentType}`)
        if (options.onError) {
          options.onError(error)
        }
        throw error
      }

      const reader = response.body?.getReader()
      if (!reader) {
        const error = new Error("æ— æ³•è·å–å“åº”è¯»å–å™¨")
        if (options.onError) {
          options.onError(error instanceof Error ? error : new Error(String(error)))
        }
        throw error
      }

      // ğŸ”¥ ä½¿ç”¨è·¨å¹³å°å…¼å®¹çš„æ–‡æœ¬è§£ç å™¨
      const decoder = createCrossPlatformTextDecoder()
      let buffer = ""
      let processedLines = 0

      safeCrossPlatformLog('log', `å¼€å§‹å¤„ç†æµå¼æ•°æ®`)

      // å¤„ç†æµ
      let lastEventType: string | null = null;
      while (true) {
        const { done, value } = await reader.read()
        if (done) {
          safeCrossPlatformLog('log', `æµå¤„ç†å®Œæˆ`, { processedLines })
          break
        }

        // ğŸ”¥ ä½¿ç”¨è·¨å¹³å°å…¼å®¹çš„è§£ç å¤„ç†
        buffer += decoder.decode(value, { stream: true })

        // ğŸ”¥ ä½¿ç”¨è·¨å¹³å°å…¼å®¹çš„è¡Œåˆ†å‰²å¤„ç†
        const { lines, remainingBuffer } = processStreamLines(buffer)
        buffer = remainingBuffer

        for (const line of lines) {
          processedLines++

          if (line.startsWith("event:")) {
            lastEventType = line.substring(6).trim() || "unknown";
            safeCrossPlatformLog('log', `æ£€æµ‹åˆ°äº‹ä»¶ç±»å‹`, { eventType: lastEventType })
            continue;
          }

          if (line.startsWith("data:")) {
            const data = line.substring(5).trim();
            if (data === "[DONE]") {
              safeCrossPlatformLog('log', `æ”¶åˆ°æµå¼ç»“æŸæ ‡è®°`)
              continue
            }

            try {
              const parsed = data ? JSON.parse(data) : {};
              // åªæœ‰ eventType å­˜åœ¨æ—¶æ‰å¤„ç†
              if (lastEventType) {
                logFastGPTEvent(lastEventType, parsed);
                switch (lastEventType) {
                  case "answer":
                  case "fastAnswer":
                    if (parsed.choices && parsed.choices.length > 0 && parsed.choices[0].delta) {
                      const textChunk = parsed.choices[0].delta.content || ""
                      if (textChunk && options.onChunk) {
                        options.onChunk(textChunk)
                      }
                    }
                    break;
                  case "flowNodeStatus":
                  case "moduleStatus":
                  case "moduleStart":
                  case "moduleEnd":
                  case "thinking":
                  case "thinkingStart":
                  case "thinkingEnd":
                  case "toolCall":
                  case "toolParams":
                  case "toolResponse":
                  case "updateVariables":
                    if (options.onProcessingStep) {
                      const step = {
                        id: `${lastEventType}-${Date.now()}`,
                        type: lastEventType,
                        name: typeof parsed === 'object' && parsed !== null && 'name' in parsed ? parsed.name : (typeof parsed === 'object' && parsed !== null && 'moduleName' in parsed ? parsed.moduleName : lastEventType),
                        status: typeof parsed === 'object' && parsed !== null && 'status' in parsed ? parsed.status : "running",
                        content: typeof parsed === 'string' ? parsed : (typeof parsed === 'object' && parsed !== null && 'content' in parsed ? parsed.content : (typeof parsed === 'object' && parsed !== null && 'text' in parsed ? parsed.text : JSON.stringify(parsed))),
                        details: parsed,
                        timestamp: new Date(),
                      }
                      logProcessingStep(step)
                      options.onProcessingStep(step)
                    }
                    if (options.onIntermediateValue) {
                      options.onIntermediateValue(parsed, lastEventType)
                    }
                    break;
                  case "interactive":
                    // ğŸ”¥ å¢å¼ºäº¤äº’èŠ‚ç‚¹äº‹ä»¶å¤„ç†
                    safeCrossPlatformLog('log', `æ£€æµ‹åˆ°äº¤äº’èŠ‚ç‚¹äº‹ä»¶`, { parsed, eventType: lastEventType });
                    if (options.onIntermediateValue) {
                      options.onIntermediateValue(parsed, lastEventType);
                    }
                    break;
                  default:
                    // ğŸ”¥ å¢å¼ºé»˜è®¤äº‹ä»¶å¤„ç†
                    safeCrossPlatformLog('log', `å¤„ç†å…¶ä»–äº‹ä»¶`, { eventType: lastEventType, parsed });
                    if (options.onIntermediateValue) {
                      options.onIntermediateValue(parsed, lastEventType)
                    }
                    break;
                }
                lastEventType = null; // åªæ¶ˆè´¹ä¸€æ¬¡
              } else {
                // ğŸ”¥ å¢å¼ºæ— äº‹ä»¶ç±»å‹çš„æ•°æ®å¤„ç†
                safeCrossPlatformLog('warn', `æ”¶åˆ°æ— äº‹ä»¶ç±»å‹çš„æ•°æ®ï¼Œå°è¯•ç›´æ¥è§£æ`, {
                  dataPreview: data.substring(0, 100)
                });
                // å°è¯•ä½œä¸ºæ™®é€šçš„æµå¼å“åº”å¤„ç†
                if (parsed.choices && parsed.choices.length > 0 && parsed.choices[0].delta) {
                  const textChunk = parsed.choices[0].delta.content || ""
                  if (textChunk && options.onChunk) {
                    options.onChunk(textChunk)
                  }
                }
              }
            } catch (e) {
              safeCrossPlatformLog('error', `è§£æSSEæ•°æ®è¡Œå‡ºé”™`, {
                error: e,
                dataPreview: data.substring(0, 100)
              })
              // ğŸ”¥ å¢å¼ºé”™è¯¯æ¢å¤ï¼Œå°è¯•ä½œä¸ºçº¯æ–‡æœ¬å¤„ç†
              if (data && options.onChunk && !data.includes('{')) {
                safeCrossPlatformLog('log', `å°è¯•ä½œä¸ºçº¯æ–‡æœ¬å¤„ç†`)
                options.onChunk(data)
              }
            }
          }
        }
      }

      // ç¡®ä¿åœ¨æ­£å¸¸é€€å‡ºå¾ªç¯æ—¶è°ƒç”¨ onFinish
      if (options.onFinish) {
        options.onFinish()
      }
    } catch (error) {
      clearTimeout(timeoutId)

      // ğŸ”¥ ä½¿ç”¨è·¨å¹³å°å…¼å®¹çš„é”™è¯¯åˆ†ç±»
      const errorInfo = categorizeStreamError(error)
      safeCrossPlatformLog('error', `æµå¼èŠå¤©è¯·æ±‚é”™è¯¯`, {
        errorType: errorInfo.type,
        errorMessage: errorInfo.message,
        shouldRetry: errorInfo.shouldRetry,
        originalError: error
      })

      // è®°å½•é”™è¯¯
      logError("FastGPT API", error instanceof Error ? error : new Error(String(error)), { apiEndpoint, requestBody })

      // ğŸ”¥ æ ¹æ®é”™è¯¯ç±»å‹æä¾›ä¸åŒçš„å¤„ç†
      if (options.onError) {
        options.onError(error instanceof Error ? error : new Error(String(error)))
      } else {
        // å¦‚æœæ²¡æœ‰é”™è¯¯å¤„ç†å™¨ï¼Œç”Ÿæˆä¸€ä¸ªç®€å•çš„å“åº”
        if (options.onChunk) {
          const errorMessage = errorInfo.shouldRetry ?
            `è¿æ¥é—®é¢˜ï¼š${errorInfo.message}ï¼Œå»ºè®®é‡è¯•` :
            `è¯·æ±‚å¤±è´¥ï¼š${errorInfo.message}`
          options.onChunk(errorMessage)
        }

        if (options.onFinish) {
          options.onFinish()
        }
      }

      throw error

      // ä¸æŠ›å‡ºé”™è¯¯ï¼Œè€Œæ˜¯é™é»˜å¤„ç†ï¼Œä»¥é¿å…ä¸­æ–­ç”¨æˆ·ä½“éªŒ
      console.log("ä½¿ç”¨ç¦»çº¿æ¨¡å¼ç»§ç»­æœåŠ¡")
    }
  }

  /**
   * å‘é€éæµå¼èŠå¤©è¯·æ±‚
   */
  async chat(messages: any[], options: StreamOptions): Promise<string> {
    // æ£€æŸ¥APIç«¯ç‚¹æ˜¯å¦é…ç½®
    const apiEndpoint = API_CONSTANTS.FASTGPT_API_ENDPOINT
    if (!apiEndpoint) {
      console.error("API endpoint is not configured")
      return "æŠ±æ­‰ï¼ŒAPIç«¯ç‚¹æœªé…ç½®ã€‚è¯·è”ç³»ç®¡ç†å‘˜è·å–å¸®åŠ©ã€‚"
    }

    // å¦‚æœAPIå¯†é’¥æˆ–AppIDæœªé…ç½®ï¼Œè¿”å›ç¦»çº¿å“åº”
    if (!this.agent.apiKey || !this.agent.appId) {
      console.warn("API key or AppID is not configured, using offline response")
      return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ã€‚è¯·ç¡®ä¿æ‚¨å·²é…ç½®APIå¯†é’¥å’ŒAppIDï¼Œæˆ–è”ç³»ç®¡ç†å‘˜è·å–å¸®åŠ©ã€‚"
    }

    const requestBody = {
      model: this.agent.appId,
      chatId: this.agent.chatId,
      messages,
      variables: options.variables || {},
      ...options
    }

    const res = await fetchWithRetry(apiEndpoint, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json', 'Authorization': `Bearer ${this.agent.apiKey}` },
      body: JSON.stringify(requestBody),
    })

    if (!res.ok) {
      const errorText = await res.text()
      throw new Error(errorText)
    }

    const response = await res.json()
    return response.choices[0].message.content
  }

  /**
   * è·å–é—®é¢˜å»ºè®®
   */
  async getQuestionSuggestions(customConfig?: {
    open?: boolean
    model?: string
    customPrompt?: string
  }): Promise<string[]> {
    const response = await getQuestionSuggestions(
      this.agent,
      this.agent.chatId || generateFallbackChatId(),
      customConfig,
    )
    return response.data
  }
}

// ================ å‡½æ•°å¼API ================

/**
 * åˆå§‹åŒ–èŠå¤©ä¼šè¯
 */
export async function initializeChat(agent: Agent, chatId?: string): Promise<ChatInitResponse> {
  // ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„APIç«¯ç‚¹
  const apiEndpoint = API_CONSTANTS.FASTGPT_API_ENDPOINT

  // æ£€æŸ¥APIç«¯ç‚¹æ˜¯å¦é…ç½®ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨å›é€€å“åº”
  if (!apiEndpoint) {
    console.error("API endpoint is not configured")
    return generateFallbackResponse(agent, chatId || generateFallbackChatId())
  }

  // å¦‚æœAPIå¯†é’¥æˆ–AppIDæœªé…ç½®ï¼Œä½¿ç”¨å›é€€å“åº”ä½†ä¸é˜»æ­¢åˆå§‹åŒ–
  if (!agent.apiKey || !agent.appId) {
    console.warn("API key or AppID is not configured, using fallback response")
    return generateFallbackResponse(agent, chatId || generateFallbackChatId())
  }

  try {
    // Use a more reliable URL construction approach
    const apiUrl = API_CONSTANTS.FASTGPT_BASE_API

    // Construct the target URL
    const targetUrl = `${apiUrl}/core/chat/init?appId=${agent.appId}${chatId ? `&chatId=${chatId}` : ""}`

    console.log(`Initializing chat with URL: ${targetUrl}`)

    // Add a timeout to the fetch request
    const controller = new AbortController()
    const timeoutId = setTimeout(() => controller.abort(), API_CONSTANTS.INIT_TIMEOUT)

    try {
      let response

      if (shouldUseProxy()) {
        // Use the proxy API
        const proxyUrl = `/api/chat-proxy?targetUrl=${encodeURIComponent(targetUrl)}`

        response = await fetch(proxyUrl, {
          method: "GET",
          headers: {
            "Content-Type": "application/json",
            Authorization: `Bearer ${agent.apiKey}`,
          },
          signal: controller.signal,
          cache: "no-store",
        })

        clearTimeout(timeoutId)

        if (!response.ok) {
          console.error(`Proxy request failed with status: ${response.status}`)
          throw new Error(`Proxy request failed: ${response.status}`)
        }

        // Check content type before parsing as JSON
        const contentType = response.headers.get("content-type")
        if (!contentType || !contentType.includes("application/json")) {
          console.error(`Unexpected content type: ${contentType}`)
          // Try to get the response text for better error diagnosis
          const text = await response.text()
          console.error(`Response body (first 200 chars): ${text.substring(0, 200)}`)
          throw new Error(`API returned non-JSON response: ${contentType || "unknown content type"}`)
        }

        const proxyResponse = await response.json().catch((error) => {
          console.error("JSON parsing error:", error)
          throw new Error(`Failed to parse JSON response: ${error.message}`)
        })

        if (proxyResponse.status !== 200) {
          console.error(`API request failed with status: ${proxyResponse.status}`)
          throw new Error(`API request failed: ${proxyResponse.status}`)
        }

        return proxyResponse.data
      } else {
        // Direct API call (server-side only)
        response = await fetch(targetUrl, {
          method: "GET",
          headers: {
            "Content-Type": "application/json",
            Authorization: `Bearer ${agent.apiKey}`,
          },
          signal: controller.signal,
          cache: "no-store",
        })

        clearTimeout(timeoutId)

        if (!response.ok) {
          console.error(`API request failed with status: ${response.status}`)
          throw new Error(`API request failed: ${response.status}`)
        }

        // Check content type before parsing as JSON
        const contentType = response.headers.get("content-type")
        if (!contentType || !contentType.includes("application/json")) {
          console.error(`Unexpected content type: ${contentType}`)
          // Try to get the response text for better error diagnosis
          const text = await response.text()
          console.error(`Response body (first 200 chars): ${text.substring(0, 200)}`)
          throw new Error(`API returned non-JSON response: ${contentType || "unknown content type"}`)
        }

        return await response.json().catch((error) => {
          console.error("JSON parsing error:", error)
          throw new Error(`Failed to parse JSON response: ${error.message}`)
        })
      }
    } catch (fetchError) {
      // Check specifically for network errors
      if (fetchError instanceof TypeError && fetchError.message.includes("fetch")) {
        console.error("Network error during chat initialization - operating in offline mode:", fetchError)
      } else {
        console.error("Fetch error during chat initialization:", fetchError)
      }
      throw fetchError // Re-throw to be caught by the outer try-catch
    }
  } catch (error) {
    console.error("Chat initialization error:", error)

    // Always generate a fallback response with a chatId
    const fallbackChatId = chatId || generateFallbackChatId()
    console.log("Using fallback chatId:", fallbackChatId)

    // Return a valid fallback response that won't cause further errors
    return generateFallbackResponse(agent, fallbackChatId)
  }
}

// fallback å“åº”ç±»å‹è½¬æ¢
function toChatCompletionResponse(obj: any): ChatCompletionResponse {
  return {
    id: obj.id,
    object: 'chat.completion',
    created: obj.created,
    model: obj.model,
    choices: obj.choices,
    usage: obj.usage,
  };
}

/**
 * å‘é€èŠå¤©è¯·æ±‚
 */
export async function sendChatRequest(
  agent: Agent,
  messages: ChatCompletionRequest['messages'],
  options: {
    stream?: boolean
    temperature?: number
    maxTokens?: number
    detail?: boolean
    onChunk?: (chunk: string) => void
  } = {},
): Promise<ChatCompletionResponse> {
  // 1. å‚æ•°æ ¡éªŒ
  const apiEndpoint = agent.apiEndpoint || API_CONSTANTS.FASTGPT_API_ENDPOINT

  // æ£€æŸ¥APIç«¯ç‚¹æ˜¯å¦é…ç½®
  if (!apiEndpoint) {
    const errMsg = "API ç«¯ç‚¹æœªé…ç½®ï¼Œè¯·è”ç³»ç®¡ç†å‘˜ã€‚"
    if (options.stream && options.onChunk) options.onChunk(errMsg)
    return toChatCompletionResponse(generateFallbackChatResponse(errMsg, agent.multimodalModel || "unknown"))
  }

  // å¦‚æœAPIå¯†é’¥æˆ–AppIDæœªé…ç½®ï¼Œä½¿ç”¨ç¦»çº¿æ¨¡å¼ä½†ä¸é˜»æ­¢è¯·æ±‚
  if (!agent.apiKey || !agent.appId) {
    const errMsg = "API å¯†é’¥æˆ– AppID æœªé…ç½®ï¼Œä½¿ç”¨ç¦»çº¿æ¨¡å¼ã€‚"
    console.warn(errMsg)

    // å¦‚æœæ˜¯æµå¼è¯·æ±‚ï¼Œå‘é€ç¦»çº¿å“åº”
    if (options.stream && options.onChunk) {
      options.onChunk("æŠ±æ­‰ï¼Œæˆ‘æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ã€‚è¯·ç¡®ä¿æ‚¨å·²é…ç½®APIå¯†é’¥å’ŒAppIDï¼Œæˆ–è”ç³»ç®¡ç†å‘˜è·å–å¸®åŠ©ã€‚")
      return toChatCompletionResponse(generateFallbackChatResponse(errMsg, agent.multimodalModel || "unknown"))
    }

    // å¦‚æœæ˜¯éæµå¼è¯·æ±‚ï¼Œè¿”å›ç¦»çº¿å“åº”
    return toChatCompletionResponse(generateFallbackChatResponse("æŠ±æ­‰ï¼Œæˆ‘æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ã€‚è¯·ç¡®ä¿æ‚¨å·²é…ç½®APIå¯†é’¥å’ŒAppIDï¼Œæˆ–è”ç³»ç®¡ç†å‘˜è·å–å¸®åŠ©ã€‚", agent.multimodalModel || "unknown"))
  }

  // 2. è‡ªåŠ¨ç”Ÿæˆ chatId
  if (!agent.chatId) {
    agent.chatId = generateFallbackChatId()
  }

  // 3. æ‹¼æ¥ä¸Šä¸‹æ–‡å†å²ï¼Œè‡ªåŠ¨è£å‰ªï¼ˆå¦‚ 6 è½®ï¼‰
  let formattedMessages = messages.map((msg) => ({
    role: msg.role,
    content: typeof msg.content === "string" ? msg.content : JSON.stringify(msg.content),
  }))
  // æ³¨å…¥ systemPrompt
  if (agent.systemPrompt && !formattedMessages.some((msg) => msg.role === "system")) {
    formattedMessages.unshift({ role: "system", content: agent.systemPrompt })
  }
  // è£å‰ªå†å²ï¼Œé˜²æ­¢ token è¶…é™ï¼ˆå¦‚åªä¿ç•™æœ€å 12 æ¡ï¼‰
  if (formattedMessages.length > 12) {
    formattedMessages = [formattedMessages[0], ...formattedMessages.slice(-11)]
  }

  // 4. æ„é€ è¯·æ±‚ä½“
  const requestBody = {
    model: agent.appId,
    messages: formattedMessages,
    stream: options.stream === true,
    detail: options.detail ?? true,
    temperature: options.temperature ?? agent.temperature ?? 0.8,
    max_tokens: options.maxTokens ?? agent.maxTokens ?? 2048,
    user: undefined, // å¯æ‰©å±•
    // å…¶å®ƒå‚æ•°å¦‚ tools/response_mode/stop å¯æŒ‰éœ€æ‰©å±•
  }

  // 5. è¯·æ±‚å¤´
  const headers: Record<string, string> = {
    "Content-Type": "application/json",
    Authorization: `Bearer ${agent.apiKey}`,
  }
  if (options.stream) headers["Accept"] = "text/event-stream"

  // 6. æµå¼/éæµå¼è¯·æ±‚
  try {
    if (options.stream && options.onChunk) {
      // æµå¼è¾“å‡º
      const response = await fetch(apiEndpoint, {
        method: "POST",
        headers,
        body: JSON.stringify(requestBody),
      })
      if (!response.ok) {
        const errorText = await response.text()
        options.onChunk(`APIè¯·æ±‚å¤±è´¥: ${response.status} ${errorText}`)
        throw new Error(errorText)
      }
      const reader = response.body?.getReader()
      const decoder = new TextDecoder()
      let content = ""
      let buffer = ""
      while (reader) {
        const { done, value } = await reader.read()
        if (done) break
        buffer += decoder.decode(value, { stream: true })
        const lines = buffer.split("\n")
        buffer = lines.pop() || ""
        for (const line of lines) {
          if (line.trim() === "") continue
          if (line.startsWith("data: ")) {
            const data = line.slice(6).trim()
            if (data === "[DONE]") continue
            try {
              const parsed = JSON.parse(data)
              const chunk = parsed.choices?.[0]?.delta?.content || ""
              if (chunk) {
                content += chunk
                options.onChunk(chunk)
              }
            } catch (e) {
              // å¿½ç•¥è§£æé”™è¯¯
            }
          }
        }
      }
      return {
        id: `chatcmpl-${Date.now()}`,
        object: "chat.completion",
        created: Math.floor(Date.now() / 1000),
        model: agent.appId,
        choices: [
          {
            index: 0,
            message: { role: "assistant", content },
            finish_reason: "stop",
          },
        ],
        usage: { prompt_tokens: 0, completion_tokens: 0, total_tokens: 0 },
      } as ChatCompletionResponse;
    } else {
      // éæµå¼
      const response = await fetch(apiEndpoint, {
        method: "POST",
        headers,
        body: JSON.stringify(requestBody),
      })
      if (!response.ok) {
        const errorText = await response.text()
        throw new Error(errorText)
      }
      return await response.json() as ChatCompletionResponse;
    }
  } catch (error: any) {
    const errMsg = error?.message || "APIè¯·æ±‚å¼‚å¸¸"
    if (options.stream && options.onChunk) options.onChunk(errMsg)
    return toChatCompletionResponse(generateFallbackChatResponse(errMsg, agent.multimodalModel || "unknown"))
  }
}

const CACHE_TTL = 5 * 60; // 5åˆ†é’Ÿç¼“å­˜ï¼Œå•ä½ï¼šç§’

/**
 * è·å–é—®é¢˜å»ºè®®
 */
export async function getQuestionSuggestions(
  agent: Agent,
  chatId: string,
  customConfig?: {
    open?: boolean
    model?: string
    customPrompt?: string
  },
): Promise<QuestionGuideResponse> {
  // åªåšçº¯ç®—æ³•å®ç°ï¼Œä¸æŸ¥/å†™ redis
  const apiEndpoint = API_CONSTANTS.FASTGPT_API_ENDPOINT;

  // æ£€æŸ¥APIç«¯ç‚¹æ˜¯å¦é…ç½®
  if (!apiEndpoint) {
    console.error("API endpoint is not configured")
    return {
      code: 200,
      data: getDefaultSuggestions(),
    };
  }

  // å¦‚æœAPIå¯†é’¥æˆ–AppIDæœªé…ç½®ï¼Œè¿”å›é»˜è®¤å»ºè®®ä½†ä¸é˜»æ­¢åŠŸèƒ½
  if (!agent.apiKey || !agent.appId) {
    console.warn("API key or AppID is not configured, using default suggestions")
    return {
      code: 200,
      data: getDefaultSuggestions(),
    };
  }
  try {
    const baseUrl = API_CONSTANTS.FASTGPT_BASE_API;
    const targetUrl = `${baseUrl}/core/ai/agent/v2/createQuestionGuide`;
    const requestBody = {
      appId: agent.appId,
      chatId: chatId,
      questionGuide: {
        open: true,
        model: agent.multimodalModel || "GPT-4o-mini",
        customPrompt: customConfig?.customPrompt || "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç”ŸæˆçŒœä½ æƒ³é—®ã€‚",
      },
    };
    const response = await fetch(targetUrl, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${agent.apiKey}`,
      },
      body: JSON.stringify(requestBody),
    });
    if (!response.ok) {
      return {
        code: 200,
        data: getDefaultSuggestions(),
      };
    }
    const result = await response.json();
    return result;
  } catch (error) {
    return {
      code: 200,
      data: getDefaultSuggestions(),
    };
  }
}

// 1. fetchWithRetry å·¥å…·
export async function fetchWithRetry(url: string, options: RequestInit = {}, retries = 3, backoff = 300): Promise<Response> {
  let lastError
  for (let i = 0; i < retries; i++) {
    try {
      const res = await fetch(url, options)
      if (!res.ok) throw new Error(res.statusText)
      return res
    } catch (err) {
      lastError = err
      await new Promise(r => setTimeout(r, backoff * Math.pow(2, i)))
    }
  }
  throw lastError
}

// 2. streamChat æ”¯æŒæµå¼æ–­çº¿é‡è¿
// åœ¨ streamChat å†…éƒ¨ï¼Œè‹¥æµæ–­å¼€ä¸”æœªè¶…æœ€å¤§é‡è¿æ¬¡æ•°ï¼Œè‡ªåŠ¨é‡è¿å¹¶æ‹¼æ¥å†…å®¹
