/**
 * è¯­éŸ³å½•éŸ³æ ¸å¿ƒ Hook
 * å¤„ç†å½•éŸ³é€»è¾‘ã€éŸ³é¢‘å¤„ç†å’ŒçŠ¶æ€ç®¡ç†
 */

import { useState, useRef, useCallback, useEffect } from 'react'
import { 
  VoiceConfig, 
  VoiceRecordingState, 
  UseVoiceRecorderReturn,
  VOICE_CONSTANTS 
} from '@/types/voice'
import { 
  getRecordingOptions, 
  getBestAudioFormat 
} from '@/lib/voice/config'
import { 
  handleMediaError, 
  handleRecorderError, 
  createVoiceError,
  VOICE_ERROR_CODES 
} from '@/lib/voice/errors'

/**
 * è¯­éŸ³å½•éŸ³ Hook
 */
export function useVoiceRecorder(config: VoiceConfig): UseVoiceRecorderReturn {
  // çŠ¶æ€ç®¡ç†
  const [state, setState] = useState<VoiceRecordingState>({
    isRecording: false,
    isProcessing: false,
    duration: 0,
    audioLevel: 0,
    error: null,
    isReady: false,
    stream: null
  })

  // å¼•ç”¨ç®¡ç†
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const mediaStreamRef = useRef<MediaStream | null>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const chunksRef = useRef<Blob[]>([])
  const timerRef = useRef<NodeJS.Timeout | null>(null)
  const animationRef = useRef<number | null>(null)
  const startTimeRef = useRef<number>(0)
  const durationIntervalRef = useRef<NodeJS.Timeout | null>(null)

  /**
   * æ¸…ç†èµ„æº
   */
  const cleanup = useCallback(() => {
    // åœæ­¢å½•éŸ³
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop()
    }

    // åœæ­¢åª’ä½“æµ
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => track.stop())
      mediaStreamRef.current = null
    }

    // å…³é—­éŸ³é¢‘ä¸Šä¸‹æ–‡
    if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
      audioContextRef.current.close()
      audioContextRef.current = null
    }

    // æ¸…ç†å®šæ—¶å™¨
    if (timerRef.current) {
      clearInterval(timerRef.current)
      timerRef.current = null
    }

    // æ¸…ç†åŠ¨ç”»å¸§
    if (animationRef.current) {
      cancelAnimationFrame(animationRef.current)
      animationRef.current = null
    }

    // æ¸…ç†å¼•ç”¨
    mediaRecorderRef.current = null
    analyserRef.current = null
    chunksRef.current = []

    // æ¸…ç†å½•éŸ³æ—¶é•¿å®šæ—¶å™¨
    if (durationIntervalRef.current) {
      clearInterval(durationIntervalRef.current)
      durationIntervalRef.current = null
    }

    // æ¸…ç†æµ
    mediaStreamRef.current = null
    state.stream = null
  }, [])

  /**
   * æ›´æ–°éŸ³é¢‘çº§åˆ«
   */
  const updateAudioLevel = useCallback(() => {
    if (!analyserRef.current) return

    const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount)
    analyserRef.current.getByteFrequencyData(dataArray)

    // è®¡ç®—å¹³å‡éŸ³é‡
    const average = dataArray.reduce((sum, value) => sum + value, 0) / dataArray.length
    const normalizedLevel = Math.min(average / 128, 1) // å½’ä¸€åŒ–åˆ° 0-1

    setState(prev => ({
      ...prev,
      audioLevel: normalizedLevel,
    }))

    if (state.isRecording) {
      animationRef.current = requestAnimationFrame(updateAudioLevel)
    }
  }, [state.isRecording])

  /**
   * å¼€å§‹å½•éŸ³
   */
  const startRecording = useCallback(async (): Promise<void> => {
    try {
      console.log('ğŸ¤ å¼€å§‹å½•éŸ³...')
      
      setState(prev => ({
        ...prev,
        error: null,
        isReady: false,
      }))

      // æ£€æŸ¥é…ç½®
      if (!config.enabled) {
        throw createVoiceError(
          VOICE_ERROR_CODES.CONFIG_MISSING,
          'è¯­éŸ³åŠŸèƒ½æœªå¯ç”¨',
          'è¯·åœ¨è®¾ç½®ä¸­å¯ç”¨è¯­éŸ³åŠŸèƒ½'
        )
      }

      // è·å–åª’ä½“æµ
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: config.sampleRate || 16000,
          channelCount: 1
        }
      })

      mediaStreamRef.current = stream
      chunksRef.current = []

      // åˆ›å»ºéŸ³é¢‘ä¸Šä¸‹æ–‡ç”¨äºå¯è§†åŒ–
      try {
        const AudioContext = window.AudioContext || (window as any).webkitAudioContext
        audioContextRef.current = new AudioContext()
        
        const source = audioContextRef.current.createMediaStreamSource(stream)
        const analyser = audioContextRef.current.createAnalyser()
        
        analyser.fftSize = 256
        analyser.smoothingTimeConstant = 0.8
        
        source.connect(analyser)
        analyserRef.current = analyser
      } catch (error) {
        console.warn('Failed to create audio context for visualization:', error)
      }

      // åˆ›å»º MediaRecorder
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      })

      mediaRecorderRef.current = mediaRecorder

      // è®¾ç½®äº‹ä»¶ç›‘å¬å™¨
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          chunksRef.current.push(event.data)
          console.log('ğŸ“¦ å½•éŸ³æ•°æ®å—:', event.data.size, 'bytes')
        }
      }

      mediaRecorder.onstop = () => {
        console.log('â¹ï¸ å½•éŸ³åœæ­¢')
        // æ¸…ç†å®šæ—¶å™¨
        if (durationIntervalRef.current) {
          clearInterval(durationIntervalRef.current)
          durationIntervalRef.current = null
        }
      }

      mediaRecorder.onerror = (event) => {
        console.error('âŒ å½•éŸ³é”™è¯¯:', event)
        setState(prev => ({
          ...prev,
          error: 'å½•éŸ³è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯',
          isRecording: false
        }))
      }

      // å¼€å§‹å½•éŸ³
      mediaRecorder.start(100) // æ¯100msæ”¶é›†ä¸€æ¬¡æ•°æ®
      startTimeRef.current = Date.now()

      // å¼€å§‹è®¡æ—¶
      durationIntervalRef.current = setInterval(() => {
        const duration = Math.floor((Date.now() - startTimeRef.current) / 1000)
        setState(prev => ({ ...prev, duration }))

        // æ£€æŸ¥æœ€å¤§å½•éŸ³æ—¶é•¿
        if (duration >= (config.maxDuration || 60)) {
          console.log('â° è¾¾åˆ°æœ€å¤§å½•éŸ³æ—¶é•¿ï¼Œè‡ªåŠ¨åœæ­¢')
          stopRecording()
        }
      }, 1000)

      setState(prev => ({
        ...prev,
        isRecording: true,
        error: null,
        stream: stream,
        duration: 0
      }))

      // å¼€å§‹éŸ³é¢‘å¯è§†åŒ–
      if (analyserRef.current) {
        updateAudioLevel()
      }

      console.log('âœ… å½•éŸ³å¼€å§‹æˆåŠŸ')
    } catch (error: any) {
      const voiceError = handleMediaError(error)
      setState(prev => ({
        ...prev,
        error: voiceError.message,
        isRecording: false,
        isReady: false,
      }))
      cleanup()
      throw voiceError
    }
  }, [config, updateAudioLevel, cleanup])

  /**
   * åœæ­¢å½•éŸ³
   */
  const stopRecording = useCallback(async (): Promise<Blob | null> => {
    try {
      console.log('ğŸ›‘ åœæ­¢å½•éŸ³...')

      if (!mediaRecorderRef.current || !mediaStreamRef.current) {
        console.warn('âš ï¸ æ²¡æœ‰æ´»åŠ¨çš„å½•éŸ³ä¼šè¯')
        return null
      }

      return new Promise<Blob | null>((resolve) => {
        const mediaRecorder = mediaRecorderRef.current!

        // è®¾ç½®åœæ­¢äº‹ä»¶ç›‘å¬å™¨
        mediaRecorder.onstop = () => {
          console.log('ğŸ“ å¤„ç†å½•éŸ³æ•°æ®...')
          
          // åˆ›å»ºéŸ³é¢‘ Blob
          const audioBlob = new Blob(chunksRef.current, { 
            type: 'audio/webm;codecs=opus' 
          })
          
          console.log('âœ… å½•éŸ³æ•°æ®å¤„ç†å®Œæˆ:', audioBlob.size, 'bytes')

          // æ¸…ç†èµ„æº
          cleanup()
          
          resolve(audioBlob)
        }

        // åœæ­¢å½•éŸ³
        if (mediaRecorder.state === 'recording') {
          mediaRecorder.stop()
        }

        // æ›´æ–°çŠ¶æ€
        setState(prev => ({
          ...prev,
          isRecording: false,
          stream: null
        }))
      })
    } catch (error) {
      console.error('âŒ åœæ­¢å½•éŸ³å¤±è´¥:', error)
      cleanup()
      setState(prev => ({
        ...prev,
        error: error instanceof Error ? error.message : 'åœæ­¢å½•éŸ³å¤±è´¥',
        isRecording: false
      }))
      return null
    }
  }, [cleanup])

  /**
   * é‡ç½®çŠ¶æ€
   */
  const reset = useCallback(() => {
    console.log('ğŸ”„ é‡ç½®å½•éŸ³çŠ¶æ€...')
    cleanup()
    setState({
      isRecording: false,
      isProcessing: false,
      duration: 0,
      audioLevel: 0,
      error: null,
      isReady: false,
      stream: null
    })
  }, [cleanup])

  // ç»„ä»¶å¸è½½æ—¶æ¸…ç†èµ„æº
  useEffect(() => {
    return cleanup
  }, [cleanup])

  return {
    state,
    startRecording,
    stopRecording,
    reset,
    cleanup
  }
}

/**
 * æ ¼å¼åŒ–å½•éŸ³æ—¶é•¿
 */
export function formatDuration(seconds: number): string {
  const minutes = Math.floor(seconds / 60)
  const remainingSeconds = seconds % 60
  return `${minutes}:${remainingSeconds.toString().padStart(2, '0')}`
}

/**
 * è·å–å½•éŸ³çŠ¶æ€æè¿°
 */
export function getRecordingStateDescription(state: VoiceRecordingState): string {
  if (state.error) {
    return state.error
  }
  
  if (state.isProcessing) {
    return 'æ­£åœ¨å¤„ç†å½•éŸ³...'
  }
  
  if (state.isRecording) {
    return `å½•éŸ³ä¸­ ${formatDuration(state.duration)}`
  }
  
  if (state.isReady) {
    return 'å‡†å¤‡å°±ç»ª'
  }
  
  return 'ç‚¹å‡»å¼€å§‹å½•éŸ³'
}
